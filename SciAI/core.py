"""This is where users start to interact with the underlying LLM and agents."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['model', 'taskRouter', 'vote']

# %% ../nbs/00_core.ipynb 3
# install dependencies
# %pip install replicate
# %pip install gradio
# %pip install langchain

# %% ../nbs/00_core.ipynb 4
# import langchain

import os
import llm_QA


# %% ../nbs/00_core.ipynb 5
model = llm_QA.LLM("meta-llama", "TogetherAI")

# %% ../nbs/00_core.ipynb 6
# A chatbot that allows users to interact with an LLM
import gradio as gr

def taskRouter(input, history):
    prompt = input["text"]
    files = input["files"]
    messages = [history,
        {
            "role": "user",
            "content": prompt
        }
    ]

    response = model.get_llm_response(messages)
    #history = [messages, 
    #    {
    #        "role": "assistant",
    #        "content": response
    #    }
    #]
    return response

def vote(data: gr.LikeData):
    if data.liked:
        print("You upvoted this response: " + data.value["value"])
    else:
        print("You downvoted this response: " + data.value["value"])

with gr.Blocks() as sciChat:
    chatbot = gr.Chatbot(
        height=300,
        type="messages",
        placeholder="<strong>Your Personal Science AI Assistant</strong><br>Ask Me Anything")
    chatbot.like(vote, None, None)
    gr.ChatInterface(
        fn=taskRouter,
        type="messages",
        chatbot=chatbot,
        textbox=gr.MultimodalTextbox(placeholder="How can I help?", container=False, scale=7),
        title="SciAI Assistant",
        description="Ask me any question or carry out a task",
        theme="soft",
        examples=[{"text": "Hello", "files": []},{"text": "What do you see in this image?", "files": []}],
        cache_examples=True,
        multimodal=True
    )
    
sciChat.launch(share=True)
